{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "\n",
    "from rlcard.agents import DQNAgent, RandomAgent\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def block():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "    \n",
    "def unblock():\n",
    "    sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Uno environment\n",
    "env = rlcard.make('uno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/uno_dqn_result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters for Deep-Q learning Agent\n",
    "replay_memory_size = 5000\n",
    "epsilon_decay_steps = 10000\n",
    "learning_rate = 0.0005\n",
    "batch_size = 32\n",
    "mlp_layers = [128, 128]\n",
    "\n",
    "# Adjusting parameters to see how it affects the learning curve\n",
    "replay_memory_size = replay_memory_size * 2\n",
    "epsilon_decay_steps = epsilon_decay_steps\n",
    "learning_rate = learning_rate / 10\n",
    "batch_size = batch_size * 2\n",
    "mlp_layers = mlp_layers * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "                 num_actions=env.num_actions,\n",
    "                 state_shape=env.state_shape[0],\n",
    "                 mlp_layers=mlp_layers,\n",
    "                 replay_memory_size=replay_memory_size,\n",
    "                 update_target_estimator_every=100,\n",
    "                 epsilon_decay_steps=epsilon_decay_steps,\n",
    "                 learning_rate=learning_rate,\n",
    "                 batch_size=batch_size,\n",
    "                 device=device,\n",
    "                 save_path=log_dir,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of players\n",
    "num_players = 2\n",
    "\n",
    "# Set the players in the environment\n",
    "env.set_agents([agent] + [RandomAgent(env.num_actions) for _ in range(num_players - 1)])\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "To train the agent, you will need to create a training loop. During each iteration of the loop, the agent makes a decision, the environment is updated, and the agent learns from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_rewards(trajectories, payoffs):\n",
    "    adjusted_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        adjusted_traj = []\n",
    "        for state, action, reward, next_state, done in traj:\n",
    "            # Actual game state details\n",
    "            raw_obs = state['raw_obs']\n",
    "            \n",
    "            # Retrieve the number of cards in player's hand\n",
    "            num_cards_player = len(raw_obs['hand'])\n",
    "            \n",
    "            # Provide the number of cards for each player with the current player being index 0\n",
    "            num_cards_opponent = raw_obs['num_cards'][1] if raw_obs['current_player'] == 0 else raw_obs['num_cards'][0]\n",
    "\n",
    "            # Adjust rewards for action cards based on the opponent's hand size\n",
    "            action_card_reward_multiplier = max(1, (7 - num_cards_opponent) / 7)\n",
    "\n",
    "            # Multiplier to give larger reward if the agent has fewer cards\n",
    "            winning_factor = (7 - num_cards_player) / 7 \n",
    "\n",
    "            if action == 60:  # Draw a card\n",
    "                reward -= max(1, 3 - num_cards_player / 7)\n",
    "\n",
    "            # Reward for playing action cards \n",
    "            if action in range(10, 15) or action in range(25, 30) or action in range(40, 45) or action in range(55, 60):\n",
    "                reward += 2 * action_card_reward_multiplier * winning_factor\n",
    "\n",
    "            # On top of the previous reward, add extra for playing action cards that makes the opponent draw cards\n",
    "            if action in [12, 14, 27, 29, 42, 44, 57, 59]:\n",
    "                reward += 2 * action_card_reward_multiplier * winning_factor\n",
    "\n",
    "            # If there's only one card left in agent's hand, add big reward because it's very likely to win if it saves the wild card for the last\n",
    "            if num_cards_player == 1 and action in [13, 14, 28, 29, 43, 44, 58, 59]:\n",
    "                # However, we don't want to encourage the agent to save the wild card if the opponent has a small hand so we divide the reward\n",
    "                reward += 25 / action_card_reward_multiplier\n",
    "\n",
    "            # Smaller reward for playing normal cards, adjusted based on the number of cards in the player's hand\n",
    "            if action in range(0, 10) or action in range(15, 25) or action in range(30, 40) or action in range(45, 55):\n",
    "                reward += 1 + (3 - num_cards_player / 7) * winning_factor\n",
    "\n",
    "            # Incentivize the agent to have fewer cards in hand\n",
    "            if (num_cards_player < num_cards_opponent):\n",
    "                reward += 1 * winning_factor\n",
    "\n",
    "            adjusted_traj.append((state, action, reward, next_state, done))\n",
    "        adjusted_trajectories.append(adjusted_traj)\n",
    "    return adjusted_trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the simulation\n",
    "\n",
    "Previously, the code would run an entire/complete simulation. However, this restricts the ability to learn from moves within the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 190/50000 [00:14<1:04:49, 12.81it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m trajectories[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     24\u001b[0m     block()\n\u001b[1;32m---> 25\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     unblock()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m evaluate_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rlcard\\agents\\dqn_agent.py:139\u001b[0m, in \u001b[0;36mDQNAgent.feed\u001b[1;34m(self, ts)\u001b[0m\n\u001b[0;32m    137\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_t \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_memory_init_size\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tmp\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m tmp\u001b[38;5;241m%\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rlcard\\agents\\dqn_agent.py:205\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    202\u001b[0m state_batch, action_batch, reward_batch, next_state_batch, done_batch, legal_actions_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Calculate best next actions using Q-network (Double DQN)\u001b[39;00m\n\u001b[1;32m--> 205\u001b[0m q_values_next \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_nograd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m legal_actions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\rlcard\\agents\\dqn_agent.py:379\u001b[0m, in \u001b[0;36mEstimator.predict_nograd\u001b[1;34m(self, s)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    378\u001b[0m     s \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 379\u001b[0m     q_as \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m q_as\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "episode_num = 50000  # Number of episodes \n",
    "\n",
    "evaluate_every = 1000 # Evaluate the agent every X episodes\n",
    "evaluate_num = 100  # Number of games played in evaluation\n",
    "\n",
    "with Logger(log_dir) as logger:\n",
    "    for episode in tqdm(range(episode_num)):  # Number of episodes\n",
    "\n",
    "        trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "        # Assuming 'payoffs' are the game outcomes for each player\n",
    "        for i, payoff in enumerate(payoffs):\n",
    "            if payoff > 0:  # Assuming a positive payoff means winning\n",
    "                payoffs[i] = 100\n",
    "            else:\n",
    "                payoffs[i] = -75\n",
    "\n",
    "        trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "        # After reorganizing the trajectories, adjust the rewards\n",
    "        trajectories = adjust_rewards(trajectories, payoffs)\n",
    "\n",
    "        for ts in trajectories[0]:\n",
    "            block()\n",
    "            agent.feed(ts)\n",
    "            unblock()\n",
    "        \n",
    "        if episode % evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    episode,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        evaluate_num,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "    # Get the paths\n",
    "    csv_path, fig_path = logger.csv_path, logger.fig_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Agent\n",
    "After training, evaluate your agent's performance. You can use RLCard's tournament function to play the game multiple times and see how well your agent performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "csv_path, fig_path = './experiments/uno_dqn_result/performance.csv', './experiments/uno_dqn_result/fig.png'\n",
    "plot_curve(csv_path, fig_path, \"dqn\")\n",
    "\n",
    "# Save model\n",
    "save_path = os.path.join(log_dir, 'model.pth')\n",
    "torch.save(agent, save_path)\n",
    "print('Model saved in', save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
