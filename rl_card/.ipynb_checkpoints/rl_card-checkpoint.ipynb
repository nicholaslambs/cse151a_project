{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "import rlcard\n",
    "\n",
    "from rlcard.agents import DQNAgent, RandomAgent\n",
    "from rlcard.utils import (\n",
    "    get_device,\n",
    "    set_seed,\n",
    "    tournament,\n",
    "    reorganize,\n",
    "    Logger,\n",
    "    plot_curve,\n",
    ")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Uno environment\n",
    "env = rlcard.make('uno')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The paths for saving the logs and learning curves\n",
    "log_dir = './experiments/uno_dqn_result/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(\n",
    "                 num_actions=env.num_actions,\n",
    "                 state_shape=env.state_shape[0],\n",
    "                 mlp_layers=[128,128], #changed to 128\n",
    "                 replay_memory_size=5000,\n",
    "                 update_target_estimator_every=100,\n",
    "                 epsilon_decay_steps=10000,\n",
    "                 learning_rate=0.0005,\n",
    "                 batch_size=32,\n",
    "                 device=device,\n",
    "                 save_path=log_dir,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of players\n",
    "num_players = 2\n",
    "\n",
    "# Set the players in the environment\n",
    "env.set_agents([agent] + [RandomAgent(env.num_actions) for _ in range(num_players - 1)])\n",
    "\n",
    "# Reset the environment\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "To train the agent, you will need to create a training loop. During each iteration of the loop, the agent makes a decision, the environment is updated, and the agent learns from the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward function for static moves on action cards, playing and drawing cards\n",
    "def adjust_rewards(trajectories, payoffs):\n",
    "    adjusted_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        adjusted_traj = []\n",
    "        for state, action, reward, next_state, done in traj:\n",
    "            if action == 60:  # Draw a card\n",
    "                reward -= 1  # Penalty for drawing a card\n",
    "\n",
    "            elif action >= 0 and action <= 9:  # Red number cards\n",
    "                reward += 1\n",
    "            elif action >= 10 and action <= 12:  # Red action cards\n",
    "                reward += 3\n",
    "            elif action == 13:  # Red wild card\n",
    "                reward += 6\n",
    "            elif action == 14:  # Red wild and draw 4 card\n",
    "                reward += 10\n",
    "\n",
    "            elif action >= 15 and action <= 24:  # Green number cards\n",
    "                reward += 1\n",
    "            elif action >= 25 and action <= 27:  # Green action cards\n",
    "                reward += 3\n",
    "            elif action == 28:  # Green wild card\n",
    "                reward += 6\n",
    "            elif action == 29:  # Green wild and draw 4 card\n",
    "                reward += 10\n",
    "\n",
    "            elif action >= 30 and action <= 39:  # Blue number cards\n",
    "                reward += 1\n",
    "            elif action >= 40 and action <= 42:  # Blue action cards\n",
    "                reward += 3\n",
    "            elif action == 43:  # Blue wild card\n",
    "                reward += 6\n",
    "            elif action == 44:  # Blue wild and draw 4 card\n",
    "                reward += 10\n",
    "\n",
    "            elif action >= 45 and action <= 54:  # Yellow number cards\n",
    "                reward += 1\n",
    "            elif action >= 55 and action <= 57:  # Yellow action cards\n",
    "                reward += 3\n",
    "            elif action == 58:  # Yellow wild card\n",
    "                reward += 6\n",
    "            elif action == 59:  # Yellow wild and draw 4 card\n",
    "                reward += 10\n",
    "                \n",
    "            adjusted_traj.append((state, action, reward, next_state, done))\n",
    "        adjusted_trajectories.append(adjusted_traj)\n",
    "    return adjusted_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_rewards(trajectories, payoffs):\n",
    "    adjusted_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        adjusted_traj = []\n",
    "        for state, action, reward, next_state, done in traj:\n",
    "            # Actual game state details\n",
    "            raw_obs = state['raw_obs']\n",
    "            \n",
    "            # Retrieve the number of cards in player's hand\n",
    "            num_cards_player = len(raw_obs['hand'])\n",
    "            \n",
    "            # Provide the number of cards for each player with the current player being index 0\n",
    "            num_cards_opponent = raw_obs['num_cards'][1] if raw_obs['current_player'] == 0 else raw_obs['num_cards'][0]\n",
    "\n",
    "            if action == 60:  # Draw a card\n",
    "                reward -= max(1, 3 - num_cards_player / 7)\n",
    "\n",
    "            # Adjust rewards for action cards based on the opponent's hand size\n",
    "            action_card_reward_multiplier = max(1, (7 - num_cards_opponent) / 7)\n",
    "\n",
    "            if action in range(10, 15) or action in range(25, 30) or action in range(40, 45) or action in range(55, 60):\n",
    "                reward += 2 * action_card_reward_multiplier\n",
    "\n",
    "            if action in range(0, 10) or action in range(15, 25) or action in range(30, 40) or action in range(45, 55):\n",
    "                reward += 1 + (3 - num_cards_player / 7)\n",
    "\n",
    "            adjusted_traj.append((state, action, reward, next_state, done))\n",
    "        adjusted_trajectories.append(adjusted_traj)\n",
    "    return adjusted_trajectories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building out the simulation\n",
    "\n",
    "Previously, the code would run an entire/complete simulation. However, this restricts the ability to learn from moves within the simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "  episode      |  0\n",
      "  reward       |  -0.14\n",
      "----------------------------------------\n",
      "INFO - Step 100, rl-loss: 44.512813568115234\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 200, rl-loss: 281.67529296875252\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 300, rl-loss: 7.5817928314208988\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 400, rl-loss: 599.04895019531255\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 500, rl-loss: 780.27404785156253\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 600, rl-loss: 11.222894668579102\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 700, rl-loss: 997.96765136718755\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 800, rl-loss: 19.003885269165045\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 900, rl-loss: 8.3345584869384774\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1000, rl-loss: 18.532949447631836\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1100, rl-loss: 245.64466857910156\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1200, rl-loss: 226.11337280273438\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1300, rl-loss: 21.298179626464844\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1400, rl-loss: 46.330028533935558\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1500, rl-loss: 24.286676406860355\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1600, rl-loss: 201.48196411132812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1700, rl-loss: 27.467458724975586\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1800, rl-loss: 40.575050354003906\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 1900, rl-loss: 46.949684143066406\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2000, rl-loss: 27.164381027221688\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2100, rl-loss: 211.86132812557812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2200, rl-loss: 37.539569854736336\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2300, rl-loss: 25.818355560302734\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2400, rl-loss: 244.37884521484375\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2500, rl-loss: 191.05596923828125\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2600, rl-loss: 21.320152282714844\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2700, rl-loss: 237.09353637695312\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2800, rl-loss: 27.802906036376953\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 2900, rl-loss: 47.509338378906255\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3000, rl-loss: 199.89813232421875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3100, rl-loss: 44.684616088867198\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3200, rl-loss: 54.098686218261725\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3300, rl-loss: 57.449699401855475\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3400, rl-loss: 59.064041137695314\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3500, rl-loss: 19.543270111083984\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3600, rl-loss: 205.43170166015625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3700, rl-loss: 31.800970077514655\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3800, rl-loss: 66.662254333496115\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 3900, rl-loss: 45.988342285156254\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4000, rl-loss: 72.645820617675782\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4100, rl-loss: 35.143497467041016\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4200, rl-loss: 40.081825256347656\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4300, rl-loss: 208.05703735351562\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4400, rl-loss: 56.484573364257815\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4500, rl-loss: 73.124061584472665\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4600, rl-loss: 56.750289916992196\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4700, rl-loss: 191.40463256835938\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4800, rl-loss: 74.555877685546885\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 4900, rl-loss: 66.963531494140622\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5000, rl-loss: 66.490592956542978\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5100, rl-loss: 74.467712402343754\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5200, rl-loss: 147.72225952148438\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5300, rl-loss: 128.57301330566406\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5400, rl-loss: 35.847915649414062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5500, rl-loss: 153.15942382812584\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5600, rl-loss: 48.435272216796875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5700, rl-loss: 66.463966369628982\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5800, rl-loss: 197.15090942382812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 5900, rl-loss: 246.53732299804688\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6000, rl-loss: 39.745323181152344\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6100, rl-loss: 88.546386718751875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6200, rl-loss: 71.819931030273446\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6300, rl-loss: 228.48696899414062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6400, rl-loss: 153.63385009765625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6500, rl-loss: 103.31320190429688\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6600, rl-loss: 332.87591552734375\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6700, rl-loss: 64.522254943847665\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6800, rl-loss: 213.20362854003906\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 6900, rl-loss: 98.101432800292972\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7000, rl-loss: 132.71997070312512\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7100, rl-loss: 124.67055511474612\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7200, rl-loss: 77.585205078125875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7300, rl-loss: 311.84930419921875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7400, rl-loss: 88.115127563476565\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7500, rl-loss: 62.573066711425784\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7600, rl-loss: 124.29756164550781\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7700, rl-loss: 210.60867309570312\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7800, rl-loss: 446.30264282226568\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 7900, rl-loss: 84.647277832031256\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8000, rl-loss: 259.39181518554696\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8100, rl-loss: 520.57659912109382\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8200, rl-loss: 218.97708129882812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8300, rl-loss: 234.47595214843754\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8400, rl-loss: 70.672103881835948\n",
      "INFO - Copied model parameters to target network.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Step 8500, rl-loss: 234.37683105468758\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8600, rl-loss: 237.64244079589844\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8700, rl-loss: 159.80555725097656\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8800, rl-loss: 274.11502075195318\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 8900, rl-loss: 97.048835754394535\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9000, rl-loss: 179.57290649414062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9100, rl-loss: 132.47454833984375\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9200, rl-loss: 196.17831420898438\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9300, rl-loss: 115.76449584960938\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9400, rl-loss: 244.60256958007812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9500, rl-loss: 71.537559509277342\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9600, rl-loss: 235.17282104492188\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9700, rl-loss: 115.74420166015625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9800, rl-loss: 436.08306884765625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 9900, rl-loss: 201.60977172851562\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10000, rl-loss: 67.87547302246094\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10100, rl-loss: 117.40605163574219\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10200, rl-loss: 130.14814758300788\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10300, rl-loss: 148.56129455566406\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10400, rl-loss: 89.732826232910162\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10500, rl-loss: 88.185821533203126\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10600, rl-loss: 182.86112976074226\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10700, rl-loss: 550.69238281254375\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10800, rl-loss: 578.30908203125688\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 10900, rl-loss: 440.33551025390625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11000, rl-loss: 173.19915771484375\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11100, rl-loss: 222.30102539062594\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11200, rl-loss: 172.62205505371094\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11300, rl-loss: 79.276840209960942\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11400, rl-loss: 146.17074584960938\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11500, rl-loss: 148.00955200195312\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11600, rl-loss: 242.65704345703125\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11700, rl-loss: 139.86157226562581\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11800, rl-loss: 140.34799194335938\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 11900, rl-loss: 204.86373901367188\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12000, rl-loss: 124.94311523437562\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12100, rl-loss: 367.58502197265625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12200, rl-loss: 101.63969421386719\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12300, rl-loss: 185.26431274414062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12400, rl-loss: 136.02935791015625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12500, rl-loss: 111.59669494628906\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12600, rl-loss: 120.21211242675781\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12700, rl-loss: 458.49594116210946\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12800, rl-loss: 206.81599426269536\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 12900, rl-loss: 167.76461791992188\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13000, rl-loss: 197.81172180175782\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13100, rl-loss: 174.03671264648438\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13200, rl-loss: 186.07711791992188\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13300, rl-loss: 168.64675903320312\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13400, rl-loss: 199.54589843753281\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13500, rl-loss: 112.98793029785156\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13600, rl-loss: 189.82745361328125\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13700, rl-loss: 201.55528259277344\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13800, rl-loss: 113.02063751220703\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 13900, rl-loss: 333.01986694335945\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14000, rl-loss: 184.85014343261722\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14100, rl-loss: 278.42791748046875\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14200, rl-loss: 116.32534027099612\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14300, rl-loss: 244.00639343261728\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14400, rl-loss: 140.11474609375756\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14500, rl-loss: 199.24758911132812\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14600, rl-loss: 158.38867187553125\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14700, rl-loss: 68.197174072265625\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14800, rl-loss: 147.30850219726562\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 14900, rl-loss: 206.35415649414062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 15000, rl-loss: 139.35220336914062\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 15100, rl-loss: 374.28222656251094\n",
      "INFO - Copied model parameters to target network.\n",
      "INFO - Step 15138, rl-loss: 209.68807983398438"
     ]
    }
   ],
   "source": [
    "episode_num = 25000  # Number of episodes \n",
    "\n",
    "evaluate_every = 1000 # Evaluate the agent every X episodes\n",
    "evaluate_num = 100  # Number of games played in evaluation\n",
    "\n",
    "with Logger(log_dir) as logger:\n",
    "    for episode in range(episode_num):  # Number of episodes\n",
    "\n",
    "        trajectories, payoffs = env.run(is_training=True)\n",
    "\n",
    "        # Assuming 'payoffs' are the game outcomes for each player\n",
    "        for i, payoff in enumerate(payoffs):\n",
    "            if payoff > 0:  # Assuming a positive payoff means winning\n",
    "                payoffs[i] = 100\n",
    "            else:\n",
    "                payoffs[i] = -25\n",
    "\n",
    "        trajectories = reorganize(trajectories, payoffs)\n",
    "\n",
    "        # After reorganizing the trajectories, adjust the rewards\n",
    "        trajectories = adjust_rewards(trajectories, payoffs)\n",
    "        # print(trajectories[0])\n",
    "\n",
    "        for ts in trajectories[0]:\n",
    "            agent.feed(ts)\n",
    "        \n",
    "        if episode % evaluate_every == 0:\n",
    "                logger.log_performance(\n",
    "                    episode,\n",
    "                    tournament(\n",
    "                        env,\n",
    "                        evaluate_num,\n",
    "                    )[0]\n",
    "                )\n",
    "\n",
    "    # Get the paths\n",
    "    csv_path, fig_path = logger.csv_path, logger.fig_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Agent\n",
    "After training, evaluate your agent's performance. You can use RLCard's tournament function to play the game multiple times and see how well your agent performs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "plot_curve(csv_path, fig_path, \"dqn\")\n",
    "\n",
    "# Save model\n",
    "save_path = os.path.join(log_dir, 'model.pth')\n",
    "torch.save(agent, save_path)\n",
    "print('Model saved in', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
